{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6434f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ab0889fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    def __init__(self):\n",
    "        self.grid_height = 6\n",
    "        self.grid_width = 10\n",
    "        self.start = (5, 0)  # Starting at the bottom-left corner (index from 0)\n",
    "        self.goal = (5, 9)   # Goal at the bottom-right corner\n",
    "        self.state = self.start\n",
    "        self.cliff = [(5, i) for i in range(1, 9)]  # Cliff positions\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state_to_one_hot()\n",
    "\n",
    "    def step(self, action):\n",
    "        actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # (dy, dx) for each action\n",
    "        next_state = tuple(np.add(self.state, actions[action]))\n",
    "        if next_state[0] < 0 or next_state[0] >= self.grid_height or next_state[1] < 0 or next_state[1] >= self.grid_width:\n",
    "            next_state = self.state\n",
    "        done = next_state == self.goal or next_state in self.cliff\n",
    "        reward = -5 if not done else (-500 if next_state in self.cliff else 0)\n",
    "        self.state = next_state\n",
    "        return self.state_to_one_hot(), reward, done, {}\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        return state[0] * self.grid_width + state[1]\n",
    "\n",
    "    def state_to_one_hot(self):\n",
    "        state_index = self.state_to_index(self.state)\n",
    "        one_hot_state = np.zeros(self.grid_height * self.grid_width)\n",
    "        one_hot_state[state_index] = 1\n",
    "        return one_hot_state\n",
    "\n",
    "    def observation_space(self):\n",
    "        return self.grid_height * self.grid_width\n",
    "\n",
    "    def action_space(self):\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "613d5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.7  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state)\n",
    "        return action_values.max(1)[1].item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)  # Adjust the shape for batch processing\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)  # Adjust the shape for batch processing\n",
    "            reward = torch.FloatTensor([reward])\n",
    "            action = torch.LongTensor([action])\n",
    "\n",
    "            current_q_values = self.model(state)\n",
    "            current_q_value = current_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Calculate the target Q-value\n",
    "            next_q_values = self.model(next_state)\n",
    "            max_next_q_value = torch.max(next_q_values, 1)[0]\n",
    "            target_q_value = reward + self.gamma * max_next_q_value * (1 - int(done))\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = nn.functional.mse_loss(current_q_value, target_q_value)\n",
    "        \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "171dcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "agent = DQNAgent(env.observation_space(), env.action_space())\n",
    "episodes = 500\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -510, Epsilon: 1.00\n",
      "Episode: 2, Total Reward: -500, Epsilon: 1.00\n",
      "Episode: 3, Total Reward: -505, Epsilon: 1.00\n",
      "Episode: 4, Total Reward: -515, Epsilon: 1.00\n",
      "Episode: 5, Total Reward: -505, Epsilon: 1.00\n",
      "Episode: 6, Total Reward: -515, Epsilon: 1.00\n",
      "Episode: 7, Total Reward: -500, Epsilon: 1.00\n",
      "Episode: 8, Total Reward: -500, Epsilon: 1.00\n",
      "Episode: 9, Total Reward: -1330, Epsilon: 0.46\n",
      "Episode: 10, Total Reward: -500, Epsilon: 0.46\n",
      "Episode: 11, Total Reward: -3050, Epsilon: 0.04\n",
      "Episode: 12, Total Reward: -615, Epsilon: 0.03\n",
      "Episode: 13, Total Reward: -1410, Epsilon: 0.01\n",
      "Episode: 14, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 15, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 16, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 17, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 18, Total Reward: -1515, Epsilon: 0.01\n",
      "Episode: 19, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 20, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 21, Total Reward: -2000, Epsilon: 0.01\n",
      "Episode: 22, Total Reward: -2190, Epsilon: 0.01\n",
      "Episode: 23, Total Reward: -590, Epsilon: 0.01\n",
      "Episode: 24, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 25, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 26, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 27, Total Reward: -985, Epsilon: 0.01\n",
      "Episode: 28, Total Reward: -130, Epsilon: 0.01\n",
      "Episode: 29, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 30, Total Reward: -625, Epsilon: 0.01\n",
      "Episode: 31, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 32, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 33, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 34, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 35, Total Reward: -550, Epsilon: 0.01\n",
      "Episode: 36, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 37, Total Reward: -540, Epsilon: 0.01\n",
      "Episode: 38, Total Reward: -540, Epsilon: 0.01\n",
      "Episode: 39, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 40, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 41, Total Reward: -525, Epsilon: 0.01\n",
      "Episode: 42, Total Reward: -545, Epsilon: 0.01\n",
      "Episode: 43, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 44, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 45, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 46, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 47, Total Reward: -1070, Epsilon: 0.01\n",
      "Episode: 48, Total Reward: -800, Epsilon: 0.01\n",
      "Episode: 49, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 50, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 51, Total Reward: -525, Epsilon: 0.01\n",
      "Episode: 52, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 53, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 54, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 55, Total Reward: -1075, Epsilon: 0.01\n",
      "Episode: 56, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 57, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 58, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 59, Total Reward: -780, Epsilon: 0.01\n",
      "Episode: 60, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 61, Total Reward: -675, Epsilon: 0.01\n",
      "Episode: 62, Total Reward: -550, Epsilon: 0.01\n",
      "Episode: 63, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 64, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 65, Total Reward: -1015, Epsilon: 0.01\n",
      "Episode: 66, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 67, Total Reward: -910, Epsilon: 0.01\n",
      "Episode: 68, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 69, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 70, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 71, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 72, Total Reward: -540, Epsilon: 0.01\n",
      "Episode: 73, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 74, Total Reward: -660, Epsilon: 0.01\n",
      "Episode: 75, Total Reward: -575, Epsilon: 0.01\n",
      "Episode: 76, Total Reward: -1535, Epsilon: 0.01\n",
      "Episode: 77, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 78, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 79, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 80, Total Reward: -950, Epsilon: 0.01\n",
      "Episode: 81, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 82, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 83, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 84, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 85, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 86, Total Reward: -940, Epsilon: 0.01\n",
      "Episode: 87, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 88, Total Reward: -1135, Epsilon: 0.01\n",
      "Episode: 89, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 90, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 91, Total Reward: -2220, Epsilon: 0.01\n",
      "Episode: 92, Total Reward: -850, Epsilon: 0.01\n",
      "Episode: 93, Total Reward: -1675, Epsilon: 0.01\n",
      "Episode: 94, Total Reward: -710, Epsilon: 0.01\n",
      "Episode: 95, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 96, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 97, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 98, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 99, Total Reward: -680, Epsilon: 0.01\n",
      "Episode: 100, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 101, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 102, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 103, Total Reward: -1445, Epsilon: 0.01\n",
      "Episode: 104, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 105, Total Reward: -610, Epsilon: 0.01\n",
      "Episode: 106, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 107, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 108, Total Reward: -845, Epsilon: 0.01\n",
      "Episode: 109, Total Reward: -890, Epsilon: 0.01\n",
      "Episode: 110, Total Reward: -785, Epsilon: 0.01\n",
      "Episode: 111, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 112, Total Reward: -1410, Epsilon: 0.01\n",
      "Episode: 113, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 114, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 115, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 116, Total Reward: -1215, Epsilon: 0.01\n",
      "Episode: 117, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 118, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 119, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 120, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 121, Total Reward: -885, Epsilon: 0.01\n",
      "Episode: 122, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 123, Total Reward: -1125, Epsilon: 0.01\n",
      "Episode: 124, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 125, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 126, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 127, Total Reward: -560, Epsilon: 0.01\n",
      "Episode: 128, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 129, Total Reward: -1255, Epsilon: 0.01\n",
      "Episode: 130, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 131, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 132, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 133, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 134, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 135, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 136, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 137, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 138, Total Reward: -610, Epsilon: 0.01\n",
      "Episode: 139, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 140, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 141, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 142, Total Reward: -1850, Epsilon: 0.01\n",
      "Episode: 143, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 144, Total Reward: -570, Epsilon: 0.01\n",
      "Episode: 145, Total Reward: -675, Epsilon: 0.01\n",
      "Episode: 146, Total Reward: -590, Epsilon: 0.01\n",
      "Episode: 147, Total Reward: -525, Epsilon: 0.01\n",
      "Episode: 148, Total Reward: -1965, Epsilon: 0.01\n",
      "Episode: 149, Total Reward: -585, Epsilon: 0.01\n",
      "Episode: 150, Total Reward: -4575, Epsilon: 0.01\n",
      "Episode: 151, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 152, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 153, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 154, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 155, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 156, Total Reward: -575, Epsilon: 0.01\n",
      "Episode: 157, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 158, Total Reward: -895, Epsilon: 0.01\n",
      "Episode: 159, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 160, Total Reward: -605, Epsilon: 0.01\n",
      "Episode: 161, Total Reward: -1130, Epsilon: 0.01\n",
      "Episode: 162, Total Reward: -635, Epsilon: 0.01\n",
      "Episode: 163, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 164, Total Reward: -1375, Epsilon: 0.01\n",
      "Episode: 165, Total Reward: -580, Epsilon: 0.01\n",
      "Episode: 166, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 167, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 168, Total Reward: -700, Epsilon: 0.01\n",
      "Episode: 169, Total Reward: -550, Epsilon: 0.01\n",
      "Episode: 170, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 171, Total Reward: -2535, Epsilon: 0.01\n",
      "Episode: 172, Total Reward: -1445, Epsilon: 0.01\n",
      "Episode: 173, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 174, Total Reward: -515, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 175, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 176, Total Reward: -525, Epsilon: 0.01\n",
      "Episode: 177, Total Reward: -545, Epsilon: 0.01\n",
      "Episode: 178, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 179, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 180, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 181, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 182, Total Reward: -545, Epsilon: 0.01\n",
      "Episode: 183, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 184, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 185, Total Reward: -1960, Epsilon: 0.01\n",
      "Episode: 186, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 187, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 188, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 189, Total Reward: -935, Epsilon: 0.01\n",
      "Episode: 190, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 191, Total Reward: -765, Epsilon: 0.01\n",
      "Episode: 192, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 193, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 194, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 195, Total Reward: -555, Epsilon: 0.01\n",
      "Episode: 196, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 197, Total Reward: -1175, Epsilon: 0.01\n",
      "Episode: 198, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 199, Total Reward: -890, Epsilon: 0.01\n",
      "Episode: 200, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 201, Total Reward: -995, Epsilon: 0.01\n",
      "Episode: 202, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 203, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 204, Total Reward: -1615, Epsilon: 0.01\n",
      "Episode: 205, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 206, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 207, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 208, Total Reward: -525, Epsilon: 0.01\n",
      "Episode: 209, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 210, Total Reward: -1685, Epsilon: 0.01\n",
      "Episode: 211, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 212, Total Reward: -1155, Epsilon: 0.01\n",
      "Episode: 213, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 214, Total Reward: -565, Epsilon: 0.01\n",
      "Episode: 215, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 216, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 217, Total Reward: -700, Epsilon: 0.01\n",
      "Episode: 218, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 219, Total Reward: -1780, Epsilon: 0.01\n",
      "Episode: 220, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 221, Total Reward: -525, Epsilon: 0.01\n",
      "Episode: 222, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 223, Total Reward: -550, Epsilon: 0.01\n",
      "Episode: 224, Total Reward: -1790, Epsilon: 0.01\n",
      "Episode: 225, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 226, Total Reward: -910, Epsilon: 0.01\n",
      "Episode: 227, Total Reward: -555, Epsilon: 0.01\n",
      "Episode: 228, Total Reward: -540, Epsilon: 0.01\n",
      "Episode: 229, Total Reward: -705, Epsilon: 0.01\n",
      "Episode: 230, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 231, Total Reward: -1180, Epsilon: 0.01\n",
      "Episode: 232, Total Reward: -810, Epsilon: 0.01\n",
      "Episode: 233, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 234, Total Reward: -590, Epsilon: 0.01\n",
      "Episode: 235, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 236, Total Reward: -1615, Epsilon: 0.01\n",
      "Episode: 237, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 238, Total Reward: -1080, Epsilon: 0.01\n",
      "Episode: 239, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 240, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 241, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 242, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 243, Total Reward: -590, Epsilon: 0.01\n",
      "Episode: 244, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 245, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 246, Total Reward: -700, Epsilon: 0.01\n",
      "Episode: 247, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 248, Total Reward: -1125, Epsilon: 0.01\n",
      "Episode: 249, Total Reward: -1945, Epsilon: 0.01\n",
      "Episode: 250, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 251, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 252, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 253, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 254, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 255, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 256, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 257, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 258, Total Reward: -540, Epsilon: 0.01\n",
      "Episode: 259, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 260, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 261, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 262, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 263, Total Reward: -1020, Epsilon: 0.01\n",
      "Episode: 264, Total Reward: -545, Epsilon: 0.01\n",
      "Episode: 265, Total Reward: -900, Epsilon: 0.01\n",
      "Episode: 266, Total Reward: -545, Epsilon: 0.01\n",
      "Episode: 267, Total Reward: -655, Epsilon: 0.01\n",
      "Episode: 268, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 269, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 270, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 271, Total Reward: -2430, Epsilon: 0.01\n",
      "Episode: 272, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 273, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 274, Total Reward: -1230, Epsilon: 0.01\n",
      "Episode: 275, Total Reward: -1525, Epsilon: 0.01\n",
      "Episode: 276, Total Reward: -590, Epsilon: 0.01\n",
      "Episode: 277, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 278, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 279, Total Reward: -595, Epsilon: 0.01\n",
      "Episode: 280, Total Reward: -2170, Epsilon: 0.01\n",
      "Episode: 281, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 282, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 283, Total Reward: -705, Epsilon: 0.01\n",
      "Episode: 284, Total Reward: -2570, Epsilon: 0.01\n",
      "Episode: 285, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 286, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 287, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 288, Total Reward: -1040, Epsilon: 0.01\n",
      "Episode: 289, Total Reward: -1740, Epsilon: 0.01\n",
      "Episode: 290, Total Reward: -565, Epsilon: 0.01\n",
      "Episode: 291, Total Reward: -535, Epsilon: 0.01\n",
      "Episode: 292, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 293, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 294, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 295, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 296, Total Reward: -520, Epsilon: 0.01\n",
      "Episode: 297, Total Reward: -575, Epsilon: 0.01\n",
      "Episode: 298, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 299, Total Reward: -920, Epsilon: 0.01\n",
      "Episode: 300, Total Reward: -675, Epsilon: 0.01\n",
      "Episode: 301, Total Reward: -1945, Epsilon: 0.01\n",
      "Episode: 302, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 303, Total Reward: -870, Epsilon: 0.01\n",
      "Episode: 304, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 305, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 306, Total Reward: -940, Epsilon: 0.01\n",
      "Episode: 307, Total Reward: -595, Epsilon: 0.01\n",
      "Episode: 308, Total Reward: -1595, Epsilon: 0.01\n",
      "Episode: 309, Total Reward: -530, Epsilon: 0.01\n",
      "Episode: 310, Total Reward: -515, Epsilon: 0.01\n",
      "Episode: 311, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 312, Total Reward: -1170, Epsilon: 0.01\n",
      "Episode: 313, Total Reward: -875, Epsilon: 0.01\n",
      "Episode: 314, Total Reward: -695, Epsilon: 0.01\n",
      "Episode: 315, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 316, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 317, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 318, Total Reward: -1210, Epsilon: 0.01\n",
      "Episode: 319, Total Reward: -645, Epsilon: 0.01\n",
      "Episode: 320, Total Reward: -1450, Epsilon: 0.01\n",
      "Episode: 321, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 322, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 323, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 324, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 325, Total Reward: -1330, Epsilon: 0.01\n",
      "Episode: 326, Total Reward: -505, Epsilon: 0.01\n",
      "Episode: 327, Total Reward: -1870, Epsilon: 0.01\n",
      "Episode: 328, Total Reward: -510, Epsilon: 0.01\n",
      "Episode: 329, Total Reward: -500, Epsilon: 0.01\n",
      "Episode: 330, Total Reward: -500, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a7a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
